---
title: 'Adaptive Exploration for Latent-State Bandits'

# Authors
authors:
  - Jikai Jin
  - Kenneth Hung
  - Sanath Kumar Krishnamurthy
  - Baoyi Shi
  - Congshan Zhang

date: '2026-02-04T00:00:00Z'
doi: '10.48550/arXiv.2602.05139'

# Schedule page publish date (NOT publication's date).
publishDate: '2026-02-04T00:00:00Z'

# Publication type.
publication_types: ['3']

# Publication name and optional abbreviated publication name.
publication: "arXiv preprint"
publication_short: "arXiv"

abstract: "The multi-armed bandit problem is a core framework for sequential decision-making under uncertainty, but classical algorithms often fail in environments with hidden, time-varying states that confound reward estimation and optimal action selection. We address key challenges arising from unobserved confounders, such as biased reward estimates and limited state information, by introducing a family of state-model-free bandit algorithms that leverage lagged contextual features and coordinated probing strategies. These implicitly track latent states and disambiguate state-dependent reward patterns. Our methods and their adaptive variants can learn optimal policies without explicit state modeling, combining computational efficiency with robust adaptation to non-stationary rewards. Empirical results across diverse settings demonstrate superior performance over classical approaches, and we provide practical recommendations for algorithm selection in real-world applications."

# Summary. An optional shortened abstract.
summary: 'We propose adaptive, state-model-free bandit algorithms for latent-state (confounded, non-stationary) environments.'

tags:
  - Bandits
  - Sequential decision making
  - Machine learning theory

# Display this page in the Featured widget?
featured: false

# Custom links
links:
  - name: ArXiv
    url: https://arxiv.org/abs/2602.05139

url_pdf: ''
url_code: ''
url_dataset: ''
url_poster: ''
url_project: ''
url_slides: ''
url_source: ''
url_video: ''
---
